{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task\n",
    "1. News Extraction: Develop a script to scrape news articles from provided URLs. Ensure the extracted content captures the full text and headline of the articles.\n",
    "2. GenAI-driven Summarization and Topic Identification:\n",
    "- Use a GenAI platform or tool (e.g. OpenAI's GPT models, or any other LLM) to analyze the articles. Your tasks will include generating a summary that captures key points and identifying the main topics of each article.\n",
    "- The focus should be on effectively integrating and utilizing GenAI tools rather than building from scratch.\n",
    "3. Semantic Search with GenAI:\n",
    "- Store the extracted news, their GenAI-generated summaries, and topics in a vector database.\n",
    "- Implement a semantic search feature leveraging GenAI tools to interpret and find relevant articles based on user queries. This search should understand the context of the queries and match them effectively with the summaries and topics. Search should handle semantically close search terms like synonyms.\n",
    "\n",
    "\n",
    "### Part 1. Downloading a page\n",
    "\n",
    "##### Option 1: Simply download page source using Requests\n",
    "**Pros:**\n",
    "- Simple to implement. The requests library is straightforward to use and does not require additional resources like a browser.  \n",
    "\n",
    "**Cons:**\n",
    "- Does not support dynamic content, Single Page Applications (SPA), and more. The content returned is only what is present in the HTML source code, which may not be the full content of the page if JavaScript is used to load additional content.\n",
    "\n",
    "##### Option 2: Render page using selenium and extract text (Chosen)\n",
    "**Pros:**\n",
    "- Any content, including SPA, can be obtained. This method allows for the full rendering of the page, including any content loaded by JavaScript.  \n",
    "\n",
    "**Cons:**\n",
    "- Longer page retrieval time. Because a full browser is required to render the page, this method can take longer than simply downloading the page source.\n",
    "- Requires more resources. Running a browser, even a headless one, requires more system resources than simply making a request to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "def get_page_text_by_url(urls):\n",
    "    pages_text = []\n",
    "    options = Options()\n",
    "    options.set_preference('permissions.default.image', 2)\n",
    "    options.set_preference('media.autoplay.default', 0)\n",
    "    options.set_preference('permissions.default.stylesheet', 2)\n",
    "    options.headless = True\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        cleared_page_text = driver.find_element(By.TAG_NAME, \"body\").text.replace('\\n', ' ')\n",
    "        pages_text.append(cleared_page_text)\n",
    "    driver.quit()\n",
    "    \n",
    "    return pages_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Example Domain This domain is for use in illustrative examples in documents. You may use this domain in literature without prior coordination or asking for permission. More information...']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_page_text_by_url(['https://example.com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Conclusion\n",
    "I selected Selenium as the primary tool for web page downloading due to its effectiveness with dynamic content, a feature common to most modern websites. I disabled the loading of images and styles, which significantly expedited the site rendering process.  \n",
    "\n",
    "### Part 2. GenAI-driven Summarization and Topic Identification\n",
    "\n",
    "##### Option 1: Local LLM (Chosen)\n",
    "\n",
    "**Pros:**\n",
    "- **Privacy and Security:** Data never leaves your network, which can be crucial for sensitive information.\n",
    "- **Customization:** You can tailor the model to your specific needs and data.\n",
    "- **Control:** You have full control over the model, including updates and maintenance.\n",
    "\n",
    "**Cons:**\n",
    "- **Resource Intensive:** Requires significant computational resources and expertise to maintain.\n",
    "- **Update and Maintenance:** Keeping the model updated and maintained can be a complex and costly process.\n",
    "- **Scalability:** May not scale as easily as public models for large data volumes.\n",
    "\n",
    "##### Option 2: Proprietary public LLM:\n",
    "\n",
    "**Pros:**\n",
    "- **Ease of Use:** Typically easier to use and maintain as the provider handles updates and maintenance.\n",
    "- **Scalability:** Can handle large volumes of data and scale easily.\n",
    "- **Access to Latest Technology:** You get access to the latest advancements and updates in the model.\n",
    "\n",
    "**Cons:**\n",
    "- **Privacy Concerns:** Data is processed on the provider's servers, which may not be ideal for sensitive information.\n",
    "- **Customization Limitations:** There may be limitations on how much you can customize the model to your specific needs.\n",
    "- **Dependency:** You are dependent on the provider for service availability and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "SUSTEM_PROMPT_FOR_SUMMARIZATION = \"Your task is to summarize the provided text and identify the main topics. The summary should be concise, capturing only the most critical elements of the text. Limit the summary to a maximum of 500 words. The list of main topics should not exceed 5 elements. Ensure that each topic is a single, clear concept and does not contain special characters, such as hyphens. Separate topics with commas. Each topic should begin with a small letter except for names and titles. Format your response as 'Summary text: [summary text]. List of topics:[list of topics]'. Avoid any unnecessary details or tangential information in your summary.\"\n",
    "BASE_LMM_SERVER_URL = \"http://localhost:1234/v1/\"\n",
    "\n",
    "def get_ai_answer(system_prompt, message, temperature, model):\n",
    "    url = f\"{BASE_LMM_SERVER_URL}chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            { \"role\": \"system\", \"content\":  system_prompt},\n",
    "            { \"role\": \"user\", \"content\": message }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": -1,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    return requests.post(url, headers=headers, data=json.dumps(payload)).json()['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the local model 'dolphin-2.9.3-mistral-nemo-12b' as it performed very well in the quick test compared to other base models such as 'dolphin-2.9.4-llama3.1-8b' and 'qwen2.5-14b_uncensored_instruct'. The model generates a result that better fits the desired pattern and produces text of reasonably good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_summary_adn_topics_by_pattern(url, temperature = 0.8, model='dolphin-2.9.3-mistral-nemo-12b', tries = 10):\n",
    "    pattern = re.compile(\"summary text:(.*)list of topics:(.*)\", re.DOTALL | re.IGNORECASE)\n",
    "    pages_text = get_page_text_by_url(url)\n",
    "    results = []\n",
    "\n",
    "    for page_text in pages_text:\n",
    "        \n",
    "        for _ in range(tries):\n",
    "            model_answer = get_ai_answer(SUSTEM_PROMPT_FOR_SUMMARIZATION, page_text, temperature, model)\n",
    "            match = re.search(pattern, model_answer)\n",
    "            if match:\n",
    "                summary_text = match.group(1).strip()\n",
    "                list_of_topics = [topic.strip() for topic in match.group(2).split(',')]\n",
    "                if any(len(s) > 42 for s in list_of_topics):\n",
    "                    continue\n",
    "                results.append({'page_text': page_text, 'summary_text': summary_text, 'list_of_topics': list_of_topics})\n",
    "                break\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am compelling the model to produce a response in a specific format, which must include a summary text and a section listing topics. If the response doesn't conform to this structure, we attempt to regenerate it. Additionally, the model occasionally generates an inaccurate list of topics, sometimes using an incorrect delimiter or none at all. To address this, there's an extra validation step: if the length of any listed topic exceeds 42 characters, we again try to generate the response. The number 42 is somewhat arbitrary, chosen for its mythical connotations in science fiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://habr.com/en/articles/868822/',\n",
    "    'https://habr.com/en/articles/865754/',\n",
    "    'https://habr.com/en/articles/865274/',\n",
    "    'https://habr.com/en/articles/865216/',\n",
    "    'https://habr.com/en/articles/864270/',\n",
    "    'https://habr.com/en/articles/861974/',\n",
    "    'https://habr.com/en/articles/861368/',\n",
    "    'https://habr.com/en/articles/847854/',\n",
    "    'https://habr.com/en/articles/846898/',\n",
    "    'https://habr.com/en/articles/841820/'\n",
    "]\n",
    "\n",
    "dataset = get_summary_adn_topics_by_pattern(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process 10 recent articles from [https://habr.com/](https://habr.com/) and output a short sammarie and list of topics. We will not output the full text for the sake of cleanliness of the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This article explores the multifaceted strategy of RABBIT testing in software development which includes Regression Testing, Automated Testing, Black Box Testing, Beta Testing, Integration Testing, and Test-Driven Development methodologies. Each branch addresses specific aspects to ensure thorough validation from various perspectives. The approach is particularly suitable for CI/CD pipelines, complex projects, environments with frequent updates, user-centric applications, and high-risk industries.',\n",
       "  ['regression testing',\n",
       "   'automated testing',\n",
       "   'black box testing',\n",
       "   'beta testing',\n",
       "   'integration testing',\n",
       "   'test-driven development.']),\n",
       " (\"Grok AI is an artificial intelligence tool created by xAI and designed for meaningful conversations with users. Integrated into X (formerly Twitter), Grok AI offers tools for chatting, image creation, writing assistance, learning, coding help, and problem-solving. It stands out due to its humor-infused responses and easy accessibility through the X platform. Currently free but limited to 10 questions per 2-hour period, Grok AI's premium subscription is available through X Premium and Premium+ tiers.\",\n",
       "  ['Grok AI',\n",
       "   'artificial intelligence tool',\n",
       "   'xAI',\n",
       "   'Twitter integration',\n",
       "   'smart conversations',\n",
       "   'image creation',\n",
       "   'multi-use capabilities',\n",
       "   'premium subscriptions']),\n",
       " ('This article provides a detailed guide on how to bypass CAPTCHA challenges effectively in automation processes. It covers various methods such as IP rotation, User-Agent rotation, cookie management, simulating human behavior, using CAPTCHA recognition services, and combining these strategies for optimal results.',\n",
       "  ['IP Rotation',\n",
       "   'User-Agent Rotation',\n",
       "   'Cookie Management',\n",
       "   'Simulating Human Behavior',\n",
       "   'CAPTCHA Recognition Services',\n",
       "   'Hybrid Strategies.']),\n",
       " ('The robotics industry is undergoing significant transformation due to advancements in artificial intelligence (AI), making robots increasingly capable of real-time decision-making and learning. AI-driven systems are addressing practical challenges by enabling predictive maintenance and real-time defect detection. Humanoid robots are versatile, affordable, and viable options for industries beyond manufacturing such as logistics, healthcare, and customer service.',\n",
       "  ['Robotics development',\n",
       "   'Artificial Intelligence',\n",
       "   'Adaptive robotics',\n",
       "   'Manufacturing industry opportunities',\n",
       "   'Workplace safety.']),\n",
       " ('This article provides an overview of various authentication methods and helps developers and business owners understand the variety of approaches in choosing optimal solutions. It covers traditional methods such as passwords and multi-factor authentication (MFA), modern methods like biometrics and one-time passwords (OTP), and CPaaS services for OTP implementation.',\n",
       "  ['Authentication Methods',\n",
       "   'Passwords',\n",
       "   'Multi-Factor Authentication',\n",
       "   'Biometrics',\n",
       "   'One-Time Passwords',\n",
       "   'CPaaS Services',\n",
       "   'Criteria for Choosing Optimal Solution']),\n",
       " ('This article compares two CAPTCHA bypass methods (clicks and tokens) using Selenium, demonstrating their speed differences through a practical comparison. The author used Python modules to perform the comparison but had to modify them slightly due to technical challenges.',\n",
       "  ['selenium captcha bypass',\n",
       "   'tokens vs clicks',\n",
       "   'google recaptcha bypass',\n",
       "   'python modules modifications',\n",
       "   'speed comparison results']),\n",
       " ('This article provides an overview of SSH key pairs and their importance in secure authentication for developers. It explains the RSA, Ed25519, and ECDSA algorithms behind SSH keys, discusses extended security considerations including quantum computing threats, and offers tips on making informed decisions when choosing between different types of SSH keys.',\n",
       "  ['ssh key pairs',\n",
       "   'cryptographic algorithms',\n",
       "   'RSA algorithm',\n",
       "   'Ed25519 algorithm',\n",
       "   'ECDSA algorithm',\n",
       "   'security considerations',\n",
       "   'quantum computing impact.']),\n",
       " ('This text discusses the process of parsing XML into plain maps in Golang. The article provides code examples and explains how to handle opening tags, closing tags, self-closed tags, and CharData in XML decoding.',\n",
       "  ['xml parsing in go',\n",
       "   'xml structure conversion',\n",
       "   'xml unmarshalling',\n",
       "   'xml token parsing',\n",
       "   'go xml handling.']),\n",
       " ('The article discusses the development and application of modern X-ray detectors. These detectors use hybrid pixel technology to convert energy from X-rays directly into electrical signals, allowing for high-resolution digital imaging.',\n",
       "  ['x-ray detectors',\n",
       "   'hybrid pixel detectors',\n",
       "   'medipix project',\n",
       "   'semiconductor sensors',\n",
       "   'energy bands in p-n junction',\n",
       "   'X-ray interactions with sensor',\n",
       "   'spectral computed tomography',\n",
       "   'dosimetry applications',\n",
       "   \"CERN's Knowledge Transfer programs.\"]),\n",
       " ('The article explores principles from \"Clean Code\" by Robert C. Martin to improve Python coding practices, including meaningful naming conventions, functions doing one thing, unnecessary comments, error handling, DRY principle (Don\\'t Repeat Yourself), test-driven development, avoiding side effects, and command query separation.',\n",
       "  ['clean python code',\n",
       "   'meaningful naming conventions',\n",
       "   'functions doing one thing',\n",
       "   'unnecessary comments',\n",
       "   'error handling',\n",
       "   \"don't repeat yourself principle\",\n",
       "   'test-driven development',\n",
       "   'avoiding side effects',\n",
       "   'command query separation.'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(el['summary_text'], el['list_of_topics']) for el in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Conclusion\n",
    "I processed 10 articles and highlight the main ideas and a list of topics for each. The chosen model perfectly handles technical text. The context length window can be expanded to 1024000 tokens, which will allow you to process several hundred news or articles without losing context.\n",
    "\n",
    "### Part 3. Semantic Search with GenAI\n",
    "\n",
    "To address this task, I will be utilizing the 'text-embedding-nomic-embed-text-v1.5' model. I will convert the text(summary_text + list_of_topics) into a vector and store it in a vector database. For article search, I will similarly transform the user's query into a vector and then seek the closest matching vector in the database. I will not be employing boosting techniques for the list of topics, and for the MVP solution, I will be using an in-memory database.\n",
    "\n",
    "This approach will enable me to find the article that is semantically most similar to the user's query. Moreover, by using an in-memory database, I can ensure faster data retrieval and processing, which will enhance the user experience. The absence of boosting techniques will ensure that all topics are treated equally, maintaining a balanced approach to information retrieval.\n",
    "\n",
    "The 'text-embedding-nomic-embed-text-v1.5' model is chosen for its efficiency in converting text into numerical vectors, which can be easily compared and matched in a vector database. This will ensure that the system can handle large volumes of data and still provide accurate and relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "SYSTEM_PROMPT_FOR_ANSWER_QUESTION = \"\"\"\n",
    "You are a system designed to assist the user with their inquiries. An incoming request and context are fed into your system. Based on this context, you are required to provide a response. The response should be as precise and concise as possible. If the user's request does not align with the provided context, the context may be disregarded.\n",
    "The input data will be presented in the following format: User Request: [user request] Context: [context].\n",
    "The output should be presented simply as text without additional context and user query sections.\n",
    "To elaborate, your primary function is to interpret the user's request, consider the given context (if applicable), and generate a response that best answers the user's query. Your responses should be direct and to the point, ensuring that the user receives the information they need in the most efficient manner possible.\n",
    "Remember, the context is provided to help refine your response, but it is not always necessary. If the user's request is clear and does not require additional context, you can generate a response based solely on the user's request. This flexibility allows you to effectively handle a wide range of user inquiries.\n",
    "The format of the input data is standardized to ensure consistency and ease of processing. The 'User Request' field will contain the user's query, while the 'Context' field will provide any additional information that may be relevant to the request.\n",
    "\"\"\"\n",
    "\n",
    "def get_embedding_lmstudio(query, model='text-embedding-nomic-embed-text-v1.5'):\n",
    "    url = f\"{BASE_LMM_SERVER_URL}embeddings\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"input\": query\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    return requests.post(url, headers=headers, data=json.dumps(payload)).json()['data'][0]['embedding']\n",
    "\n",
    "def semantic_search(query, index):\n",
    "    query_vector = get_embedding_lmstudio(query)\n",
    "    _, I = index.search(np.array([query_vector]), 1)\n",
    "    \n",
    "    return I[0][0]\n",
    "\n",
    "def build_index(dataset):\n",
    "    index = faiss.IndexFlatL2(768)\n",
    "\n",
    "    for data in dataset:\n",
    "        combined_vector = get_embedding_lmstudio(data['summary_text'] + ' '.join(data['list_of_topics']))\n",
    "        index.add(np.array([combined_vector]))\n",
    "    \n",
    "    return index\n",
    "\n",
    "def get_best_fit_article(query, index, dataset):\n",
    "    best_fit_article_index = semantic_search(query, index)\n",
    "    \n",
    "    return dataset[best_fit_article_index] \n",
    "\n",
    "def get_RAG_answer(query, best_fit_article, temperature = 0.8, model = 'dolphin-2.9.3-mistral-nemo-12b'):\n",
    "    user_query = f\"User Request: {query}. Context:{best_fit_article['page_text']}\"\n",
    "    \n",
    "    return get_ai_answer(SYSTEM_PROMPT_FOR_ANSWER_QUESTION, user_query, temperature, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit article summary: This article provides a detailed guide on how to bypass CAPTCHA challenges effectively in automation processes. It covers various methods such as IP rotation, User-Agent rotation, cookie management, simulating human behavior, using CAPTCHA recognition services, and combining these strategies for optimal results.\n",
      "RAG answer: The best way to bypass CAPTCHA is by employing hybrid strategies that combine prevention techniques with fallback CAPTCHA-solving methods. This approach involves using techniques like rotating IP addresses and altering User-Agent strings to prevent triggers, while solving remaining CAPTCHAs as a fallback solution when necessary. It offers versatility for handling websites with varying protection levels and balances cost-effectiveness with stability and adaptability.\n"
     ]
    }
   ],
   "source": [
    "query = 'I want to bypass the captcha on the site. What is the best way for me to do this?'\n",
    "\n",
    "index = build_index(dataset)\n",
    "best_fit_article = get_best_fit_article(query, index, dataset)\n",
    "RAG_answer = get_RAG_answer(query, best_fit_article)\n",
    "\n",
    "print(f\"Best fit article summary: {best_fit_article['summary_text']}\")\n",
    "print(f\"RAG answer: {RAG_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit article summary: The article explores principles from \"Clean Code\" by Robert C. Martin to improve Python coding practices, including meaningful naming conventions, functions doing one thing, unnecessary comments, error handling, DRY principle (Don't Repeat Yourself), test-driven development, avoiding side effects, and command query separation.\n",
      "RAG answer: The basic principles of clean code for Python include meaningful naming, functions doing one thing only, avoiding unnecessary comments, proper error handling, consistent formatting, following the DRY principle (Don't Repeat Yourself), test-driven development, avoiding side effects, and adhering to command query separation. These principles help to create clear, readable, maintainable Python code that is easier to understand and modify.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'Explain the basic principles of clean code for Python.'\n",
    "best_fit_article = get_best_fit_article(query, index, dataset)\n",
    "RAG_answer = get_RAG_answer(query, best_fit_article)\n",
    "\n",
    "print(f\"Best fit article summary: {best_fit_article['summary_text']}\")\n",
    "print(f\"RAG answer: {RAG_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Conclusion\n",
    "We have managed to construct a basic model of a RAG (Retrieval-Augmented Generation) system. We processed a large text, compressed it into a brief summary, and then converted this summary into a vector. In this case, the vector turns out to be more accurate as the input is condensed information. Subsequently, we search the vector database for the most relevant article and utilize its text to refine the user's query. This approach allows us to use a pre-trained model without fine-tuning it in real-time. For instance, we can process corporate documents in this manner, enabling the assistant to utilize them. This is particularly relevant when documents are updated very frequently.\n",
    "\n",
    "#### Improvements that can be made to the current implementation include:\n",
    "- Using the OpenAI library instead of requests for more efficient and streamlined operations.\n",
    "- Searching for not just the single best document, but the nearest ones in the vector space up to a certain threshold. This would require expanding the Content Length, which in turn would demand more memory.\n",
    "- Employing models with more parameters, such as 70B instead of 12B, which could potentially improve the accuracy and effectiveness of the system. This would, however, also require more computational resources.\n",
    "- Use the topic list as a facet or as an element with a high boosting factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /home/itiv422/.local/lib/python3.10/site-packages (4.27.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/itiv422/.local/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/lib/python3/dist-packages (from selenium) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/itiv422/.local/lib/python3.10/site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: trio~=0.17 in /home/itiv422/.local/lib/python3.10/site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/itiv422/.local/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/itiv422/.local/lib/python3.10/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: sortedcontainers in /home/itiv422/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/itiv422/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: outcome in /home/itiv422/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: exceptiongroup in /home/itiv422/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/itiv422/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/itiv422/.local/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/itiv422/.local/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/itiv422/.local/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /home/itiv422/.local/lib/python3.10/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/itiv422/.local/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/itiv422/.local/lib/python3.10/site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install faiss-cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
